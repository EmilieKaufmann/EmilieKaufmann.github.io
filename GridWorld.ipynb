{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAE8kMxe6E6k"
   },
   "source": [
    "# RL with and without function approximation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XNj1_VZ2FGJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from IPython import get_ipython\n",
    "import rlberry.colab_utils.display_setup\n",
    "from rlberry.colab_utils.display_setup import show_video\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You may want to run the cell below in order to get rid of the ffmpeg warning when videos are displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "(function(on) {\n",
    "const e=$( \"<a>Setup failed</a>\" );\n",
    "const ns=\"js_jupyter_suppress_warnings\";\n",
    "var cssrules=$(\"#\"+ns);\n",
    "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
    "e.click(function() {\n",
    "    var s='Showing';  \n",
    "    cssrules.empty()\n",
    "    if(on) {\n",
    "        s='Hiding';\n",
    "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
    "    }\n",
    "    e.text(s+' warnings (click to toggle)');\n",
    "    on=!on;\n",
    "}).click();\n",
    "$(element).append(e);\n",
    "})(true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4hKBRTCh6Gk"
   },
   "source": [
    "# Preparation\n",
    "\n",
    "In the coding exercises, you will use a *grid-world* MDP, which is represented in Python using the interface provided by the [Gym](https://gym.openai.com/) library. The cells below show how to interact with this MDP and how to visualize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "514mHDeQooKa"
   },
   "outputs": [],
   "source": [
    "from rlberry.envs import GridWorld\n",
    "\n",
    "def get_env():\n",
    "  \"\"\"Creates an instance of a grid-world MDP.\"\"\"\n",
    "  env = GridWorld(\n",
    "      nrows=5,\n",
    "      ncols=7,\n",
    "      reward_at = {(0, 6):1.0},\n",
    "      walls=((0, 4), (1, 4), (2, 4), (3, 4)),\n",
    "      success_probability=0.9,\n",
    "      terminal_states=((0, 6),)\n",
    "  )\n",
    "  return env\n",
    "\n",
    "def render_policy(env, policy=None, horizon=50):\n",
    "  \"\"\"Visualize a policy in an environment\n",
    "\n",
    "  Args:\n",
    "    env: GridWorld\n",
    "        environment where to run the policy\n",
    "    policy: np.array\n",
    "        matrix mapping states to action (Ns).\n",
    "        If None, runs random policy.\n",
    "    horizon: int\n",
    "        maximum number of timesteps in the environment.\n",
    "  \"\"\"\n",
    "  env.enable_rendering()\n",
    "  state = env.reset()                       # get initial state\n",
    "  for timestep in range(horizon):\n",
    "      if policy is None:\n",
    "        action = env.action_space.sample()  # take random actions\n",
    "      else:\n",
    "        action = policy[state]\n",
    "      next_state, reward, is_terminal, info = env.step(action)\n",
    "      state = next_state\n",
    "      if is_terminal:\n",
    "        break\n",
    "  # save video and clear buffer\n",
    "  env.save_video('./videos/gw.mp4', framerate=5)\n",
    "  env.clear_render_buffer()\n",
    "  env.disable_rendering()\n",
    "  # show video\n",
    "  show_video('./videos/gw.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQAHUBw_ifMI"
   },
   "outputs": [],
   "source": [
    "# Create an environment and visualize it\n",
    "env = get_env()\n",
    "render_policy(env)  # visualize random policy\n",
    "\n",
    "# The reward function and transition probabilities can be accessed through\n",
    "# the R and P attributes:\n",
    "print(f\"Shape of the reward array = (S, A) = {env.R.shape}\")\n",
    "print(f\"Shape of the transition array = (S, A, S) = {env.P.shape}\")\n",
    "print(f\"Reward at (s, a) = (1, 0): {env.R[1, 0]}\")\n",
    "print(f\"Prob[s\\'=2 | s=1, a=0]: {env.P[1, 0, 2]}\")\n",
    "print(f\"Number of states and actions: {env.Ns}, {env.Na}\")\n",
    "\n",
    "# The states in the griworld correspond to (row, col) coordinates.\n",
    "# The environment provides a mapping between (row, col) and the index of\n",
    "# each state:\n",
    "print(f\"Index of state (1, 0): {env.coord2index[(1, 0)]}\")\n",
    "print(f\"Coordinates of state 5: {env.index2coord[5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibGD_3I89CNu"
   },
   "source": [
    "# Finding the optimal policy\n",
    "\n",
    "**Complete the code below in order to run value iteration in a tabular MDP with transition P and rewards R.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jB7dfA5nRCZ"
   },
   "outputs": [],
   "source": [
    "def value_iteration(P, R, gamma=0.95, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        P: np.array\n",
    "            transition matrix (NsxNaxNs)\n",
    "        R: np.array\n",
    "            reward matrix (NsxNa)\n",
    "        gamma: float\n",
    "            discount factor\n",
    "        tol: float\n",
    "            precision of the solution\n",
    "    Return:\n",
    "        Q: final Q-function (at iteration n)\n",
    "        greedy_policy: greedy policy wrt Qn\n",
    "    \"\"\"\n",
    "    Ns, Na = R.shape\n",
    "    Q = np.zeros((Ns, Na)) # current Q function \n",
    "    V = np.zeros(Ns) # current value function\n",
    "    \n",
    "    while True:\n",
    "        TQ = np.zeros((Ns, Na))\n",
    "        ### TO BE COMPLETED \n",
    "        V = TQ.max(axis=1)\n",
    "\n",
    "        if np.abs(TQ-Q).max() < tol:\n",
    "            break\n",
    "        Q = TQ\n",
    "    \n",
    "    greedy_policy = np.argmax(Q, axis=1)\n",
    "    # ====================================================\n",
    "    return Q, greedy_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Fi0IzZJp74Z"
   },
   "source": [
    "### Testing your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "tol = 1e-5\n",
    "gamma = 0.95\n",
    "\n",
    "# Environment\n",
    "env = get_env()\n",
    "\n",
    "# run value iteration to obtain Q-values\n",
    "VI_Q, VI_greedypol = value_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
    "\n",
    "# render the policy\n",
    "print(\"[VI]Greedy policy: \")\n",
    "print(VI_greedypol)\n",
    "render_policy(env, VI_greedypol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf51VhoPxbV4"
   },
   "source": [
    "# Collecting a database of transition\n",
    "\n",
    "To run Fitted-Q Iteration, we will need a database of transitions that convers the MDP well enough. We first compare two ways of collecting such a dataset, given in the code below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lNPhB28EcGd"
   },
   "outputs": [],
   "source": [
    "def get_random_policy_dataset(env, n_samples):\n",
    "  \"\"\"Get a dataset following a random policy to collect data.\"\"\"\n",
    "  states = []\n",
    "  actions = []\n",
    "  rewards = []\n",
    "  next_states = []\n",
    "  \n",
    "  state = env.reset()\n",
    "  for _ in range(n_samples):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, is_terminal, info = env.step(action)\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    next_states.append(next_state)\n",
    "    # update state\n",
    "    state = next_state\n",
    "    if is_terminal:\n",
    "      state = env.reset()\n",
    "\n",
    "  dataset = (states, actions, rewards, next_states)\n",
    "  return dataset\n",
    "\n",
    "def get_uniform_dataset(env, n_samples):\n",
    "  \"\"\"Get a dataset by uniformly sampling states and actions.\"\"\"\n",
    "  states = []\n",
    "  actions = []\n",
    "  rewards = []\n",
    "  next_states = []\n",
    "  for _ in range(n_samples):\n",
    "    state = env.observation_space.sample()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, is_terminal, info = env.sample(state, action)\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    next_states.append(next_state)\n",
    "\n",
    "  dataset = (states, actions, rewards, next_states)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "# Collect two different datasets\n",
    "num_samples = 500\n",
    "env = get_env()\n",
    "dataset_1 = get_random_policy_dataset(env, num_samples)\n",
    "dataset_2 = get_uniform_dataset(env, num_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the datasets that are built:\n",
    "\n",
    "1. Estimate the transitions and the rewards, $\\hat{P}$ and $\\hat{R}$.\n",
    "2. Compute the optimal value function and the optimal policy with respect to the estimated MDP (defined by $\\hat{P}$ and $\\hat{R}$), which we denote by $\\hat{\\pi}$ and $\\hat{V}$.\n",
    "3. Numerically compare the performance of $\\hat{\\pi}$ and $\\pi^\\star$ (the true optimal policy), and the error between $\\hat{V}$ and $V^*$ (the true optimal value function).\n",
    "\n",
    "Which of the two data collection methods do you think is better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mdp(dataset, Ns, Na):\n",
    "  ## TO BE COMPLETED \n",
    "  return P_hat, R_hat\n",
    "\n",
    "P_hat_1, R_hat_1 = estimate_mdp(dataset_1, env.Ns, env.Na)\n",
    "P_hat_2, R_hat_2 = estimate_mdp(dataset_2, env.Ns, env.Na)\n",
    "\n",
    "Q_true, pi_true = value_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
    "Q_hat_1, pi_hat_1= value_iteration(P_hat_1, R_hat_1, gamma=gamma, tol=tol)\n",
    "Q_hat_2, pi_hat_2 = value_iteration(P_hat_2, R_hat_2, gamma=gamma, tol=tol)\n",
    "\n",
    "print(f\"Error with random policy dataset: {np.abs(Q_hat_1-Q_true).max()}\")\n",
    "print(f\"Error with uniform dataset: {np.abs(Q_hat_2-Q_true).max()}\")\n",
    "\n",
    "render_policy(env, pi_hat_1, horizon=100)\n",
    "render_policy(env, pi_hat_2, horizon=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "542QxKsSOs21"
   },
   "source": [
    "# Larger gridworlds: Fitted-Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now consider a larger Gridworld, in which standard reinforcement learning algorithm like Q-Learning will typically take a very long time to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_large_gridworld():\n",
    "  \"\"\"Creates an instance of a grid-world MDP with more states.\"\"\"\n",
    "  walls = [(ii, 10) for ii in range(15) if (ii != 7 and ii != 8)]\n",
    "  env = GridWorld(\n",
    "      nrows=15,\n",
    "      ncols=15,\n",
    "      reward_at = {(14, 14):1.0},\n",
    "      walls=tuple(walls),\n",
    "      success_probability=0.9,\n",
    "      terminal_states=((14, 14),)\n",
    "  )\n",
    "  return env\n",
    "\n",
    "env = get_large_gridworld()\n",
    "render_policy(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a datset $(s_i, a_i, r_i, s_i')$ of (states, actions, rewards, next states), the Fitted Q-Iteration (FQI) algorithm proceeds as follows:\n",
    "\n",
    "\n",
    "* We start from a $Q$ function $Q_0 \\in \\mathcal{F}$, where $\\mathcal{F}$ is a function space;\n",
    "* At every iteration $k$, we compute $Q_{k+1}$ as:\n",
    "\n",
    "$$\n",
    "Q_{k+1}\\in\\arg\\min_{f\\in\\mathcal{F}} \\frac{1}{2}\\sum_{i=1}^N\n",
    "\\left(\n",
    "  f(s_i, a_i) - y_i^k\n",
    "\\right)^2 + \\lambda \\Omega(f)\n",
    "$$\n",
    "where $y_i^k = r_i + \\gamma \\max_{a'}Q_k(s_i', a')$, $\\Omega(f)$ is an (optional) regularization term and $\\lambda > 0$ is the regularization coefficient.\n",
    "\n",
    "\n",
    "We will implement FQI with *linear* function approximation and assume a state feature map $\\phi : S \\rightarrow \\mathbb{R}^d$. We define $\\mathcal{F}$ to be the parametric family of $Q$ functions $Q_\\theta(s,a) = \\phi(s)^T\\theta_a$ for $\\theta_a\\in\\mathbb{R}^d$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZovF3VXOVfCs"
   },
   "source": [
    "We propose below to implement the feature map as a class whose method \"map\" applied to a state $s$ return $\\phi(s)$. \n",
    "\n",
    "How are the features constructed? You may propose other features map(s) later to see their impact on the algorithm. \n",
    "\n",
    "**Complete the method \"Q_table\" which takes as an input a vector $\\theta \\in \\mathbb{R}^{d\\times A}$ parameterizing the Q-function and outputs the corresponding Q table in $\\mathbb{R}^{S \\times A}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMap:\n",
    "    def __init__(self, env, dim=15, sigma=0.25):\n",
    "        self.index2coord = env.index2coord\n",
    "        self.n_states = env.Ns\n",
    "        self.n_actions = env.Na\n",
    "        self.dim = dim\n",
    "        self.sigma = sigma\n",
    "\n",
    "        n_rows = env.nrows\n",
    "        n_cols = env.ncols\n",
    "\n",
    "        # build uniform discretization \n",
    "        m=int(np.floor(np.sqrt(self.dim)))\n",
    "        # X and Y coordinates of the centers\n",
    "        XS, YS = np.meshgrid(np.linspace(0.01,0.99, m),np.linspace(0.01,0.99, m))\n",
    "        XS=XS.flatten()\n",
    "        YS=YS.flatten()\n",
    "    \n",
    "        tot = m*m\n",
    "        while (tot < self.dim):\n",
    "            XS = np.hstack((XS,np.random.rand()))\n",
    "            YS = np.hstack((YS,np.random.rand()))\n",
    "            tot +=1\n",
    "        #print(np.size(XS))\n",
    "           \n",
    "        # build feature matrix \n",
    "        features = np.zeros((self.dim,self.n_states))\n",
    "    \n",
    "        for d in range(self.dim):\n",
    "            xd = XS[d]\n",
    "            yd = YS[d]\n",
    "            for ii in range(self.n_states):\n",
    "                row_ii, col_ii = self.index2coord[ii]\n",
    "                x_ii = row_ii / n_rows\n",
    "                y_ii = col_ii / n_cols\n",
    "                dist = np.sqrt((xd - x_ii) ** 2.0 + (yd - y_ii) ** 2.0)\n",
    "                features[d, ii] = np.exp(-(dist / sigma) ** 2.0)\n",
    "    \n",
    "        self.feats = features\n",
    "    \n",
    "    def map(self, observation):\n",
    "        feat = self.feats[:, observation].copy()\n",
    "        return feat\n",
    "\n",
    "    def Q_table(self,theta):\n",
    "        Q = np.zeros((self.n_states, self.n_actions))\n",
    "        ## TO BE COMPLETED \n",
    "        return Q\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to get a sense of what is inside a feature map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InCfu7F9-TbS"
   },
   "outputs": [],
   "source": [
    "feat_map = FeatureMap(env)\n",
    "\n",
    "# The features have dimension (feature_dim).\n",
    "feature_example = feat_map.map(1) # feature representation of s=1\n",
    "print(\"feature vector of state 1:\")\n",
    "print(feature_example)\n",
    "\n",
    "# Initial vector theta representing the Q function\n",
    "theta = np.random.rand(feat_map.dim, env.action_space.n)\n",
    "print(\"the shape of theta is\")\n",
    "print(theta.shape)\n",
    "\n",
    "print(\"values of Q(s=1,.):\")\n",
    "Q=feat_map.Q_table(theta)\n",
    "print(Q[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement Linear Fitted Q Iteration, taking as an input a feature map a number of iterations and the number of samples used in the training database (shared accross iterations).** \n",
    "\n",
    "For the regression problem, we propose to use $\\frac{1}{2}\\sum_a ||\\theta_a||_2^2$ as the regularization term. Assuming that we have a given dataset of $N$ tuples of the form $(s_i, a_i, r_i, s_i')$ and we are at the $k$-th iteration. Let $\\theta_k \\in\\mathbb{R}^{d \\times A}$ be our current parameter. You will need to derive the closed-form update to find $\\theta_{k+1}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fqi(env, feat_map, num_iterations,n_samples = 10000,lambd=0.1,gamma=0.95):\n",
    "    ## TO BE COMPLETED\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the algorithm and visualizing the value and policies that are learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_large_gridworld()\n",
    "feat_map = FeatureMap(env, dim=10, sigma=0.2)\n",
    "\n",
    "# FQI\n",
    "theta = linear_fqi()\n",
    "\n",
    "# Compute and run greedy policy\n",
    "Q_fqi = feat_map.Q_table(theta)\n",
    "V_fqi = Q_fqi.max(axis=1)\n",
    "policy = Q_fqi.argmax(axis=1)\n",
    "\n",
    "render_policy(env, policy, horizon=100)\n",
    "img = env.get_layout_img(V_fqi)\n",
    "plt.imshow(img)\n",
    "\n",
    "Q_true, pi_true = value_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
    "V_true = Q_true.max(axis=1)\n",
    "\n",
    "print(\"Error on the value:\",np.abs(V_fqi-V_true).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RL-MVA_2021-Homework_1.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
