{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook needs to be finished and uploaded on the link https://nextcloud.univ-lille.fr/index.php/s/EWEZLFEEJNfaT8s by March 3rd, 2023. Please use Name_FirstName as the name of the notebook or folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xn5vmAfVHAto"
   },
   "source": [
    "# Exploration-Exploitation in Reinforcement Learning\n",
    "\n",
    "In this tutorial, we will implement the **UCBVI** algorithm, for exploration in MDPs with finite states and actions and a **finite horizon** criterion. In a finite horizon criterion, the value function of a policy is\n",
    "\n",
    "$$V_h^{\\pi}(s) = \\mathbb{E}^{\\pi}\\left[\\left.\\sum_{\\ell = h}^{H} \\gamma^{\\ell-h} r_{\\ell} \\right| s_h = s\\right]$$\n",
    "\n",
    "where the discount parameter $\\gamma \\in (0,1]$ is often set to $\\gamma = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BJkOpohiql8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IafL1SoOnaqI"
   },
   "outputs": [],
   "source": [
    "# initialize display and import function to show videos\n",
    "from rlberry.colab_utils.display_setup import show_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1mYv6U5nbI2"
   },
   "outputs": [],
   "source": [
    "# other useful imports\n",
    "import numpy as np\n",
    "import numba\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "from rlberry.wrappers import DiscretizeStateWrapper\n",
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "(function(on) {\n",
    "const e=$( \"<a>Setup failed</a>\" );\n",
    "const ns=\"js_jupyter_suppress_warnings\";\n",
    "var cssrules=$(\"#\"+ns);\n",
    "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
    "e.click(function() {\n",
    "    var s='Showing';  \n",
    "    cssrules.empty()\n",
    "    if(on) {\n",
    "        s='Hiding';\n",
    "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
    "    }\n",
    "    e.text(s+' warnings (click to toggle)');\n",
    "    on=!on;\n",
    "}).click();\n",
    "$(element).append(e);\n",
    "})(true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsiPgClSnPzt"
   },
   "source": [
    "## Environment\n",
    "\n",
    "Our goal is to learn a good policy in a Mountain Car environment. The Mountain Car environement as implemented in gym has a continuous state space. In order to apply UCBVI, we will discretize it (using a <a href=\"https://github.com/rlberry-py/rlberry/blob/main/rlberry/wrappers/discretize_state.py\">wrapper from the rlberry library</a>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df115JX-nNDZ"
   },
   "outputs": [],
   "source": [
    "def render(env, horizon=180,policy=None):\n",
    "  \"\"\"\n",
    "  input  \n",
    "  horizon : length of the simulation \n",
    "  policy : either a determinstic policy represented by an (H,S) array \n",
    "  or a random policy which is uniform (None)\n",
    "  \"\"\"\n",
    "  env = deepcopy(env)\n",
    "  env = Monitor(env, './gym_videos', force=True, video_callable=lambda episode: True)\n",
    "  for episode in range(1):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    for hh in range(horizon):\n",
    "        if policy is not None:\n",
    "          action = policy[hh, state]\n",
    "        else:\n",
    "          action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "    env.close()\n",
    "    #show_video(directory='gym_videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "dsHuqRTiO1p4",
    "outputId": "5eedbdb0-2868-4918-92e3-d2816124bc08"
   },
   "outputs": [],
   "source": [
    "class MountainCatRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        if done:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "        done = False \n",
    "        return next_state, reward, done, info\n",
    "\n",
    "def get_mountain_car_env():\n",
    "  env_with_continuous_states = MountainCatRewardWrapper(gym.make('MountainCar-v0'))\n",
    "  env = DiscretizeStateWrapper(env_with_continuous_states, n_bins=10)\n",
    "  return env\n",
    "\n",
    "env = get_mountain_car_env()\n",
    "render(env) #saves the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = env.discretized_states[0,:] # discretized positions\n",
    "Xdot = env.discretized_states[1,:] # discretized velocities\n",
    "\n",
    "test = 67\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(\"state \",test,\" is \", (X[test],Xdot[test])) \n",
    "show_video(directory='gym_videos') # play the video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhRxLpONyenM"
   },
   "source": [
    "# Implementation of backward induction (i.e. value iteration)\n",
    "\n",
    "In a finite-horizon MDP, the optimal Bellman equations given a recursion that can be used to compute the optimal value function. We have $V_{H+1}^\\star(s) = 0$ for all $s$ and for $h \\leq H$, \n",
    "\n",
    "$$Q^\\star_h(s,a) = r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s,a) V^\\star_{h+1}(s') \\ \\ \\text{and } \\ \\ V^\\star_{h}(s) = \\max_{a \\in \\mathcal{A}} Q_h^\\star(s,a).$$\n",
    "\n",
    "Recall that the optimal policy is deterministic but *non-stationary* and satisfies $\\pi^\\star_h(s) = \\text{argmax}_{a} Q^\\star_h(s,a)$. \n",
    "\n",
    "**Complete the code below in order to compute the optimal Q function in a finite-horizon MDP.**\n",
    "\n",
    "Note that this code will also be useful to compute the policy used in each episode by UCB-VI, where we have to perform Value Iteration in an optimistic MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmDNWZ_Syc3w"
   },
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)  # use this to make the code much faster!\n",
    "\n",
    "def backward_induction(P, R, H, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        P: transition function (S,A,S)-dim matrix\n",
    "        R: reward function (S,A)-dim matrix\n",
    "        H: horizon\n",
    "        gamma: discount factor. Usually set to 1.0 in finite-horizon problems\n",
    "\n",
    "    Returns:\n",
    "        The optimal Q-function: array of shape (horizon, S,A)      \n",
    "    \"\"\"\n",
    "    S, A = P.shape[0], P.shape[1]\n",
    "    V = np.zeros((H + 1, S))\n",
    "    Q = np.zeros((H+1,S,A))\n",
    "    for h in range(H-1, -1, -1):\n",
    "        for s in range(S):\n",
    "            ## TODO: COMPUTE (Q[h,s,a])_{a} and V[h,s]\n",
    "            # ... and clip the value (needed later in UCB-VI)\n",
    "            if (V[h, s] > H - h):\n",
    "                V[h, s] = H - h\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot try this function on the moutain car environement, as the expected rewards and transition probabilities cannot be easily computed, and are not embedded in the environment, unlike in our previous GridWorld example. \n",
    "\n",
    "So you can check your code on a simple gridworld. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "MlndQUyF98As",
    "outputId": "9ee26cf3-1564-4685-c509-d3d0e39a50ca"
   },
   "outputs": [],
   "source": [
    "# Testing the implementation in a GridWorld\n",
    "from rlberry.envs import GridWorld\n",
    "\n",
    "test_env = GridWorld(nrows=8, ncols=8)\n",
    "H = 50 # pick an horizon which is sufficient to reach the goal\n",
    "\n",
    "Q_test = backward_induction(test_env.P,test_env.R,H,gamma=1.0)\n",
    "\n",
    "state = test_env.reset()\n",
    "test_env.enable_rendering()\n",
    "for h in range(H):   \n",
    "  action = np.argmax(Q_test[h, state,:])\n",
    "  next_state, reward, is_terminal, info = test_env.step(action)\n",
    "  if is_terminal:\n",
    "    break\n",
    "  state = next_state\n",
    "\n",
    "# save video (run next cell to visualize it)\n",
    "test_env.save_video('./videos/value_iteration_gw.mp4', framerate=10)\n",
    "# clear rendering data\n",
    "test_env.clear_render_buffer()\n",
    "test_env.disable_rendering()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see video\n",
    "show_video(filename='./videos/value_iteration_gw.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8xtdOayzhWL"
   },
   "source": [
    "# Implementation of UCBVI\n",
    "\n",
    "The UCBVI algorithm works as follows:\n",
    "\n",
    "* In each episode $t$, the agent has observed $n_t$ transitions $(s_i, a_i, r_i, s_{i+1})_{i=1}^{n_t}$ of states, actions, rewards and next states.\n",
    "* We estimate a model of the MDP as:\n",
    "$$\n",
    "\\mathbf{rewards:}\\quad\\widehat{R}_t(s, a) = \\frac{1}{N_t(s, a)} \\sum_{i=1}^{n_t} \\mathbb{1}\\{s = s_i, a = a_i)\\} r_i\n",
    "\\\\\n",
    "\\mathbf{transitions:}\\quad \\widehat{P}_t(s'|s, a) =  \\frac{1}{N_t(s, a)} \\sum_{i=1}^{n_t} \\mathbb{1}\\{s = s_i, a = a_i, s'=s_{i+1}\\} \n",
    "$$\n",
    "where\n",
    "$$\n",
    "N_t(s, a) = \\max\\left(1, \\sum_{i=1}^{n_t} \\mathbb{1}\\{s = s_i, a = a_i)\\} \\right)\n",
    "$$\n",
    "* We define exploration bonuses as\n",
    "$$\n",
    "B_t(s, a) \\propto \\sqrt{\\frac{1}{N_t(s, a)}} \\cdot\n",
    "$$\n",
    "\n",
    "* Then, in episode $t$, we compute $\\{Q_h^t(s, a)\\}_{h=1}^H$ as the ($H$-horizon) optimal value functions in the MDP whose transitions are $\\widehat{P}_t$ and whose rewards are $(\\widehat{R}_t + B_t)$. At step $h$ of episode $t$, the agent chooses the action $a_h^t \\in \\arg\\max_a Q_h^t(s, a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qi3Uw-mHyTbF"
   },
   "outputs": [],
   "source": [
    "# An example of bonus function\n",
    "def bonus(N): \n",
    "    \"\"\"input : a numpy array (nb of visits)\n",
    "    output : a numpy array (bonuses)\"\"\"\n",
    "    nn = np.maximum(N, 1)\n",
    "    return np.sqrt(1.0/nn)\n",
    "\n",
    "# The UCB-VI algorithm\n",
    "def UCBVI(env,H, nb_episodes,bonus_function=bonus,gamma=1):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        env: environement\n",
    "        bonus_function : maps the number of visits to the corresponding bonus\n",
    "        H: horizon\n",
    "        gamma: discount factor. Usually set to 1.0 in finite-horizon problems\n",
    "\n",
    "    Returns:\n",
    "        episode_rewards: a vector storing the sum of rewards obtained in each episode \n",
    "        N_sa : array of size (S,A) giving the total number of visits in each state\n",
    "        Rhat : array of size (S,A) giving the estimated average rewards\n",
    "        Phat : array of size (S,A,S) giving the estimated transition probabilities\n",
    "        optimistic_Q : array of size (H,S,A) giving the optimistic Q function used in the last episode     \n",
    "    \"\"\"\n",
    "    S = env.observation_space.n\n",
    "    A = env.action_space.n\n",
    "    ## TO BE COMPLETED\n",
    "    \n",
    "    return episode_rewards, N_sa, Rhat, Phat,optimistic_Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether the algorithm is working, your can can UCB-VI for a large number of episodes and monitor the amount of reward collected during the episode and the total number of visited states every 50 episodes. The algorithm should be improving, both in terms of maximizing rewards and in terms of exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8_u8o-1zxC0",
    "outputId": "80d269c0-209d-4e4c-8c42-9ac304c76613"
   },
   "outputs": [],
   "source": [
    "NUM_REPETITIONS = 1\n",
    "HORIZON = 180\n",
    "NUM_EPISODES = 500\n",
    "\n",
    "env = get_mountain_car_env()\n",
    "\n",
    "rewards = np.zeros((NUM_REPETITIONS, NUM_EPISODES))\n",
    "for sim in range(NUM_REPETITIONS):\n",
    "    print(f\"Running simulation: {sim}\")\n",
    "    rewards[sim], N_sa, Rhat, Phat, optimistic_Q = UCBVI(env, H=HORIZON, nb_episodes=NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of UCB-VI \n",
    "\n",
    "The theoretical guarantees for UCB-VI are in terms of regret, so the algorithm is aimed at collecting a large amount of rewards during learning. However, it can still provide a guess for the optimal policy and an estimate of its value.  \n",
    "\n",
    "Two particular policies may be of interest: \n",
    "- the optimistic policy, which is the greedy policy wrt the optimistic Q-Value in the last episode \n",
    "- the empirical optimal policy, which is the optimal policy in the empirical MDP given by Rhat and P_hat\n",
    "\n",
    "**Which one seems more interesting to you? For each policy, visualize how it behaves in the environment and display an estimate of the optimal value function. Comment on its shape.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "otxcN5dU12mN",
    "outputId": "31fcb70b-6dfd-4ba6-e736-36b354e0de3f"
   },
   "outputs": [],
   "source": [
    "policy = np.argmax(optimistic_Q,axis=2)\n",
    "render(env, HORIZON,policy)\n",
    "show_video(directory='gym_videos')\n",
    "\n",
    "value = np.max(optimistic_Q,axis=2)\n",
    "plt.scatter(env.discretized_states[0, :], env.discretized_states[1, :], c=value[0, :], s=400)\n",
    "plt.xlabel('Car Position')\n",
    "plt.ylabel('Car Velocity')\n",
    "clb=plt.colorbar()\n",
    "clb.ax.set_title('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZkPh_plB6p-"
   },
   "source": [
    "UCB-VI is supposed to be good at maximizing rewards. Now we can display the total reward obtained in $k$ episodes as a function of the episode $k$ for different variants of UCB-VI, using different bonusses (e.g. a bonus closer to a theoretically-valid one, or no bonus at all) or a stupid uniform baseline. \n",
    "\n",
    "Note that unlike what we did for bandit algorithm, we cannot compute regret as we do not have access to the optimal value function for this problem. However, it may still be good to display average rewards over multiple simulations. \n",
    "\n",
    "**Display a cumulative rewards curve. What is a good bonus for this problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "FFpy8YGgB5_O",
    "outputId": "677acef5-e17d-4e83-c0d2-9362d89bee5e"
   },
   "source": [
    "# Going further (theory)\n",
    "\n",
    "*On discount versus finite horizon.* In the previous practical sessions, we were finding an optimal policy wrt to a discounted notion of value. For a task like Mountain Car, all we care about is learning the policy that climbs up hill. We showed that choosing $H$ large enough in a finite horizon MDP does it. \n",
    "\n",
    "**What discount would you have chosen for a discounted-based algorithm?**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*On the use of exploration bonus beyond UCB-VI.* Replacing rewards by rewards + a bonus that decays with the number of visits is a generic idea that can be transposed to any online reinforcement learning algorithm. \n",
    "\n",
    "**Would this idea make sense for the Fitted-Q algorithm? In what other algorithm(s) could you have done it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further (practice)\n",
    "\n",
    "Implement another RL algorithm of your choice for this problem. Compare it to UCBVI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of RL-CentraleLille2021-Exploration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
