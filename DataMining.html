<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" media="screen" type="text/css" href="design.css" />
<title>Emilie Kaufmann</title>
</head>

  <body>
<div id="logo">
<a href="http://www.cnrs.fr/"><img src="cnrs.png" alt="Logo" width="6%" align="center" border="0" /></a>
</div>

<div id="header">
  <div id="header_left"> 
    <h2> Emilie Kaufmann </h2> 
    <p>
    CNRS Junior Researcher at CRIStAL <br/>
    emilie.kaufmann"at"univ-lille.fr <br/>
    </p>
  </div>

  <div id="header_right">
  <p>
    Inria Lille - Nord Europe<br/>
    Equipe Scool, Bureau A07<br/>
    40, avenue du Halley <br/>
    59650 Villeneuve d'Ascq, FRANCE<br/>
    +333.59.57.79.12.
  </p>
  </div>
</div>



<div id="menu">


   <div class="element_menu"> 
      <h3> <a href="index.html"> Home </a> </h3>
   </div>

   <div class="element_menu"> 
     <h3> <a href="research.html"> Research </a> </h3>
   </div>
   
   <div class="element_menu"> 
     <h3> <a href="teaching.html"> Teaching </a> </h3>
   </div>
   
  
   <div class="element_menu"> 
     <h3> <a href="CV.html"> CV </a> </h3>
   </div>


</div>

<div id="corpsrech"> 
	


<p> Le <span style="color:#5858FA">Data Mining</span> (fouille de données) est un ensemble de méthodes génériques pour extraire d'une masse de données de l'information pertinente. Ces méthodes peuvent être basées sur des outils de <span style="color:#5858FA">statistique exploratoire</span> combinés avec des outils de <span style="color:#5858FA">machine learning</span> (apprentissage automatique), que nous étudierons dans ce cours.  
</p>

<p> Le cours sera illustré de TPs effectué avec la langage Python. Les séances de TPs seront effectuées par Dorian Baudry. </p> 

<p>Pour les TPs, nous recommandons l'installation d'Anaconda et la réalisation d'un jupyter notebook. Vous pouvez suivre les instructions données <a href="https://github.com/pgermain/cours2018-Intro_a_Python"> ici</a> pour l'installation et la prise en mains de ces outils. 
</p>

<ul >
<li class="withpoint"> Cours 1: Introduction à l'apprentissage supervisé. Algorithme des k plus proches voisins. </li>
<li class="withpoint"> Cours 2: Classification optimale et méthodes d'apprentissage que l'on peut en déduire.<a href="ExempleNotebook.ipynb"> Notebook illustratif</a>.
</li>
<li class="withpoint"> TP 1: Prise en mains de Python, Classifieur des k-plus proches voisins. <a href="https://github.com/DBaudry/Teachings_Data_Mining_M1">Code</a>.
</li>
<li class="withpoint"> Cours 3: Régression linéaire, Régression logistique.<a href="ExempleNotebook2.ipynb"> Notebook illustratif</a>.
</li>
</li>
<li class="withpoint"> TP2: Méthodes de régression, sélection de variables.<a href="https://github.com/DBaudry/Teachings_Data_Mining_M1/tree/master/TP2">Code</a>.
</li>
<li class="withpoint"> Cours 4: Arbres de décision.
</li>
<li class="withpoint"> Cours 5: SVM linéaires.
</li>
<li class="withpoint"> TP3: Arbres de décision, SVM.
</li>
<li class="withpoint"> Cours 6: SVM non linéaires.
</li>
<li class="withpoint"> Cours 7: Apprentissage non supervisé. k-means, clustering hiérarchique. 
</li>
<li class="withpoint"> TP4: Apprentissage non supervisé. 
</li>
<li class="withpoint"> TP5: TP final. 
</li>
</ul>

<p></p>

<p> Quelques sujets des années précédents: <a href="2017_Exam.pdf"> Examen 2017</a>. <a href="2017_Rattrapage.pdf"> Rattrapage 2017</a>. <a href="2018_Exam.pdf"> Examen 2018</a>. <a href="2018_Rattrapage.pdf"> Rattrapage 2018</a>. <a href="2019_Exam.pdf"> Examen 2019</a>. <a href="2019_Rattrapage.pdf"> Rattrapage 2019</a>.</p>


<h3> Quelques références </h3>

<p> 
<ul >
<li class="withpoint"> Introduction au Machine Learning, Chloé-Agathe Azencott. Le livre est disponible <a href="http://cazencott.info/dotclear/public/lectures/IntroML_Azencott.pdf">en ligne</a>.
<li class="withpoint"> An Introduction to Statistical Learning, with applications in R. James, Witten, Hastie et Tibshirani. Le livre est disponible sur <a href="http://www-bcf.usc.edu/~gareth/ISL/">cette page web</a>.
<li class="withpoint"> The Elements of
Statistical Learning:
Data Mining, Inference, and Prediction. Hastie, Tibshirani et Friedman (disponible en ligne <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">ici</a>)
</li> 
<li class="withpoint"> Data Mining et statistique décisionnelle. Stéphane Tufféry 
</li> 
<li class="withpoint"> Le site <a href="https://www.math.univ-toulouse.fr/~besse/Wikistat/">Wikistats</a> 
</li> 
<li class="withpoint"> Le <a href="https://www.coursera.org/learn/machine-learning">cours en ligne de Machine Learning</a> de Andrew Ng sur Coursera 
</li> 
<li class="withpoint"> <a href="https://www.kaggle.com/datasets">Kaggle</a>, une plateforme de Data Science où vous trouverez de nombreux examples de jeux de données et d'études de ceux-ci (sous R et Python) 
</li> 
</ul>
</p>


</div>

</body>


</html>
