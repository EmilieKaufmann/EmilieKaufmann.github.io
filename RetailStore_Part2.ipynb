{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You own a bike store. During week $t$, the (random) demand is $D_t$ units. \n",
    "On Monday morning you may choose to command $A_t$ additional units: they are delivered immediately before the shop opens. For each week:\n",
    "\n",
    "  * Maintenance cost: $h$ per unit in stock left from the previous week (no maintenance is needed for newly commanded items)\n",
    "  * Command cost: $c$ for each unit ordered + $c_0$ per command\n",
    "  * Sales profit: $p$ per unit sold\n",
    "  * Constraint: \n",
    "    - your warehouse has a maximal capacity of $M$ unit (any additionnal bike gets stolen)\n",
    "    - you cannot sell bikes that you don't have in stock\n",
    "\n",
    "\n",
    "* State: number of bikes in stock left from the last week => state space $\\mathcal{S} = \\{0,\\dots, M\\}$\n",
    "* Action: number of bikes commanded at the beginning of the week => action space $\\mathcal{A} = \\{0, \\dots ,M\\}$ \n",
    "* Reward = balance of the week: if you command $a_t$ bikes,\n",
    "$$r_t = -c_0 \\mathbb{1}_{(a_t >0)}- c \\times a_t - h\\times s_t + p \\times \\min(D_t, s_t+a_t, M)$$\n",
    "* Transitions: you end the week with the number of bikes $$s_{t+1} = \\max\\big(0, \\min(M, s_t + a_t)  - D_t \\big)$$ \n",
    "\n",
    "Our goal is to maximize the discounted sum of rewards, starting from an initial stock $s_1$, that is to find a policy whose value is \n",
    "$$V^*(s_1) = \\max_{\\pi}\\mathbb{E}_{\\pi}\\left[\\sum_{s=1}^{\\infty} \\gamma^{s-1}r_s\\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 15 # stock capacity\n",
    "h = 0.3 # maintenance cost (per unit)\n",
    "c = 0.5 # ordering cost (per unit)\n",
    "c0 = 0.3 # fix delivery cost per command\n",
    "p = 1 # selling price (per unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the demand distribution \n",
    "\n",
    "We choose a (truncated) geometric distribution, for which \n",
    "$$\\mathbb{P}(D_t = m) = q(1-q)^m \\ \\ \\forall m \\in \\{0,\\dots,M-1\\}$$\n",
    "and $\\mathbb{P}(D_t = M) = 1 - \\sum_{m=0}^{M-1}\\mathbb{P}(D_t = m)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demand distribution (truncated geometric with parameter q)\n",
    "q = 0.1\n",
    "pdem = np.array([q*(1-q)**m for m in range(M+1)])\n",
    "pdem[M] = pdem[M]+1-np.sum(pdem)\n",
    "\n",
    "print(\"the average demand is \",np.sum([m*pdem[m] for m in range(M+1)]))\n",
    "\n",
    "def SimuDemand(pdem): \n",
    "    cpdem = np.cumsum(pdem)\n",
    "    i=0\n",
    "    u = rd.random()\n",
    "    while (u >cpdem[i]):\n",
    "        i = i+1\n",
    "    return i \n",
    "\n",
    "print(\"a simulated demand is \",SimuDemand(pdem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of the MDP as a gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextstate(s,a,d,M):\n",
    "    # computes the next state if the demand is d\n",
    "    return max(0,min(M,s+a) -d)\n",
    "\n",
    "def nextreward(s,a,d,M,c,c0,h,p):\n",
    "    # computes the reward if the demand is d\n",
    "    rew = -c*a - h*s + p*min(M,d,s+a)\n",
    "    if (a>0):\n",
    "        rew = rew - c0\n",
    "    return rew\n",
    "\n",
    "class StoreManagement(gym.Env):\n",
    "    \"\"\"\n",
    "    Retail Store Management environment\n",
    "    The environment defines which actions can be taken at which point and\n",
    "    when the agent receives which reward.\n",
    "    \"\"\"\n",
    "    def __init__(self,FirstState):\n",
    "        \n",
    "        # General variables defining the environment\n",
    "        self.Stock_Capacity = M\n",
    "        self.Maintenance_Cost = h\n",
    "        self.Order_Cost = c \n",
    "        self.Delivery_Cost = c0\n",
    "        self.Selling_Price = p\n",
    "        self.Demand_Distribution = pdem\n",
    "        \n",
    "        # Define the action space\n",
    "        self.action_space = gym.spaces.Discrete(self.Stock_Capacity+1)\n",
    "\n",
    "        # Define the state space (state space = observation space in this example)\n",
    "        self.observation_space = gym.spaces.Discrete(self.Stock_Capacity+1)\n",
    "\n",
    "        # current time step\n",
    "        self.curr_step = -1 # \n",
    "        \n",
    "        # initial state\n",
    "        self.state = FirstState\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        simulates a transition following an action in the current state\n",
    "        action : int\n",
    "        \"\"\"\n",
    "        self.curr_step += 1\n",
    "        # simulate the demand \n",
    "        Demand = SimuDemand(self.Demand_Distribution)\n",
    "        # compute the reward\n",
    "        reward = nextreward(self.state,action,Demand,self.Stock_Capacity,self.Order_Cost,self.Delivery_Cost,self.Maintenance_Cost,self.Selling_Price)\n",
    "        # compute the next state \n",
    "        self.state = nextstate(self.state,action,Demand,self.Stock_Capacity)\n",
    "        # return 4 elements: observation / reward / termination? (no terminal state here) / information (optional) \n",
    "        return self.state, reward, False, {}\n",
    "\n",
    "    def reset(self,InitialStock):\n",
    "        \"\"\"\n",
    "        Reset the state of the environment and returns an initial observation.\n",
    "        \"\"\"\n",
    "        self.curr_step = -1\n",
    "        self.state = InitialStock\n",
    "    \n",
    "    def _render(self, mode='human'):\n",
    "        \"\"\"optional visualization of the interaction: none here\"\"\"\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function that simulates a trajectory under a policy Pi starting from some state $s_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimulateTrajectory(T,Pi,s0):\n",
    "    \"\"\"return a vector of T successive states and a vector of T successive rewards\"\"\"\n",
    "    Rewards = np.zeros(T+1)\n",
    "    States = np.zeros(T+1)\n",
    "    env=StoreManagement(s0)\n",
    "    for t in range(T):\n",
    "        States[t]=env.state\n",
    "        action=Pi(env.state)\n",
    "        state,rew,x,y=env.step(action)\n",
    "        Rewards[t]=rew\n",
    "    return States,Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running simulations with three simple baselines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 10 # initial stock \n",
    "gamma = 0.97 # discount factor \n",
    "\n",
    "def PiUniform(s):\n",
    "    # pick uniformly at random in {0,1,...,M-s}\n",
    "    x = rd.sample(range(M+1-s),1)\n",
    "    return s+x[0]\n",
    "\n",
    "def PiConstant(s,c=3):\n",
    "    # oder a constant number of c bikes \n",
    "    return min(c,M-s)\n",
    "\n",
    "def PiThreshold(s,m1=3,m2=10):\n",
    "    # if less than m1 bikes in stock, refill it up to m2\n",
    "    action = 0\n",
    "    if (s <=m1):\n",
    "        action = (m2-s)\n",
    "    return action\n",
    "        \n",
    "\n",
    "T = int(np.log(1/(0.001*(1-gamma)))/np.log(1/gamma)) # truncation of the infinite sum\n",
    "States1,Rewards1 = SimulateTrajectory(T, PiUniform,s1)\n",
    "States2,Rewards2 = SimulateTrajectory(T, PiConstant,s1)\n",
    "States3,Rewards3 = SimulateTrajectory(T, PiThreshold,s1)\n",
    "\n",
    "# plot cumulative discounted reward (on a single run):\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(Rewards1*np.array([gamma**t for t in range(T+1)])),label=\"random policy\")\n",
    "plt.plot(np.cumsum(Rewards2*np.array([gamma**t for t in range(T+1)])),label=\"constant policy\")\n",
    "plt.plot(np.cumsum(Rewards3*np.array([gamma**t for t in range(T+1)])),label=\"threshold policy\")\n",
    "plt.xlabel('weeks')\n",
    "plt.ylabel('cumulated discounted reward')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of the stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(States3[range(100)])\n",
    "plt.xlabel('weeks')\n",
    "plt.ylabel('stock')\n",
    "plt.title('Evolution of the stock under a threshold policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "Here we assume that we work on a *known* MDP: that is, we know the demand distribution hence we are able to compute the parameters of the MDP: the transition kernel and the average reward\n",
    "\n",
    "## Computation of the transitions and rewards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition and Reward kernels\n",
    "K = np.zeros((M+1,M+1,M+1)) # K[s,s',a] = p(s' | s,a) \n",
    "avgR = np.zeros((M+1,M+1)) # avgR[s,a] = average reward received in state s when playing action a\n",
    "\n",
    "## computation: iterate over all possible states, actions, and possible demand values\n",
    "for a in range(M+1):\n",
    "    for s in range(M+1):\n",
    "        for d in range(M+1):\n",
    "            # next state and reward with demand d\n",
    "            ns = max(0,min(M,s+a) -d)\n",
    "            reward = -c*a - h*s+p*min(M,d,s+a)\n",
    "            if (a>0):\n",
    "                reward = reward - c0\n",
    "            K[s,ns,a] += pdem[d]\n",
    "            avgR[s,a] += pdem[d] * reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of deterministic policies\n",
    "\n",
    "First, by trying Monte-Carlo (that would also work for random policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the values in each state by Monte-Carlo simulation (needs many simulations to be precise)\n",
    "\n",
    "Policy=PiThreshold\n",
    "\n",
    "MC = 500 # number of Monte-Carlo simulations\n",
    "Values = np.zeros(M+1)\n",
    "\n",
    "for i in range (M+1):\n",
    "    s1=i\n",
    "    val = 0\n",
    "    for j in range(MC):\n",
    "        States,Rewards=SimulateTrajectory(T,Policy,s1)\n",
    "        val += np.sum(Rewards*np.array([gamma**t for t in range(T+1)]))\n",
    "    Values[i] = val/MC  \n",
    "\n",
    "print(\"the estimated value in each states for the threshold policy are \")\n",
    "print(Values)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then by trying the matrix inversion technique (and comparing to Monte-Carlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluatePolicy(Pi):\n",
    "    avgRPi=np.zeros((M+1,1))\n",
    "    KPi=np.zeros((M+1,M+1))\n",
    "    for s in range(M+1):\n",
    "        KPi[s,:]=K[s,:,Pi(s)] # matrix P^\\pi\n",
    "        avgRPi[s]=avgR[s,Pi(s)] # vector r^\\pi\n",
    "    V = (np.dot(np.linalg.inv(np.eye(M+1) - gamma * KPi), avgRPi)).transpose()\n",
    "    return V[0]\n",
    "\n",
    "Policy=PiThreshold\n",
    "Values = EvaluatePolicy(Policy)\n",
    "\n",
    "print(Values)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the threshold policy and the constant policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Values1 = EvaluatePolicy(PiConstant)\n",
    "Values2 = EvaluatePolicy(PiThreshold)\n",
    "\n",
    "plt.plot(Values1,label =\"Constant policy c=3\")\n",
    "plt.plot(Values2,label = \"Threshold policy m1=4, m2=10\")\n",
    "plt.xlabel('stock')\n",
    "plt.ylabel('value ')\n",
    "plt.legend()\n",
    "plt.title('Evolution of the stock under a constant policy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Optimal Policy: Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy improvement\n",
    "def Improve(V):\n",
    "    '''returns the greedy policy wrt to V'''\n",
    "    Pi = np.zeros(M+1) # improved policy \n",
    "    newV = np.zeros(M+1)\n",
    "    # compute the Q table \n",
    "    Q = np.zeros((M+1,M+1))\n",
    "    for s in range(M+1):\n",
    "        for a in range(M+1):\n",
    "            Q[s,a]=avgR[s,a]+gamma*np.sum([K[s,ns,a]*V[ns] for ns in range(M+1)])\n",
    "        # improvement (greedy policy wrt to Q)\n",
    "        pi = np.argmax(Q[s,:])\n",
    "        Pi[s]=pi\n",
    "        newV[s]=Q[s,pi]\n",
    "        Pi=Pi.astype(int)\n",
    "    return Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement policy iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyIteration():\n",
    "    # initalization \n",
    "    Pi = np.zeros(M+1)\n",
    "    V = np.zeros(M+1)\n",
    "    # new policy \n",
    "    newPi = np.random.randint(M+1,size=M+1) \n",
    "    nIt = 0 \n",
    "    while (not (Pi==newPi).all()):\n",
    "        nIt +=1 \n",
    "        Pi = np.copy(newPi)\n",
    "        # evaluate the policy (transformed into a function)\n",
    "        def PiFun(s):\n",
    "            return Pi[s]\n",
    "        V = EvaluatePolicy(PiFun)\n",
    "        newPi = Improve(V)\n",
    "    return Pi,V,nIt\n",
    "        \n",
    "start = time.time()\n",
    "Pi,V,nIt = PolicyIteration()\n",
    "elapsed = time.time()-start\n",
    "\n",
    "print(\"Optimal policy is\",Pi,\"with value \",V,\" in \",nIt,\" iterations and t=\",elapsed,\" seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the optimal policy and optimal value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Pi)\n",
    "plt.xlabel('stock')\n",
    "plt.ylabel('order under the optimal policy')\n",
    "plt.title(\"Optimal Policy\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(V)\n",
    "plt.xlabel('stock')\n",
    "plt.ylabel('value of the optimal policy')\n",
    "plt.title(\"Optimal Value\")\n",
    "\n",
    "Vstar=np.copy(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Methods\n",
    "\n",
    "We now use *reinforcement learning methods*, which are not allowed to have access to the transition and rewards. They can only observe *trajectories* (sequences of transitions). \n",
    "\n",
    "## Stochastic Approximation for Policy Evaluation: TD(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD0(Pi, T):\n",
    "    V = np.random.rand(M+1) # V[s] = estimated value of each state under policy pi\n",
    "    N = np.zeros(M+1) # N[s] =number of visits to state s in the loop\n",
    "    s0 = np.random.randint(M+1)\n",
    "    env=StoreManagement(s0)\n",
    "    for t in range(T):\n",
    "        S = env.state\n",
    "        N[S] += 1\n",
    "        action = Pi(S)\n",
    "        nS,rew,x,y = env.step(action) \n",
    "        alpha = 1/((1+N[S])**0.5) \n",
    "        V[S] = (1-alpha)*V[S] + alpha * (rew + gamma*V[nS]) \n",
    "    return(V)\n",
    "\n",
    "T = 10**7\n",
    "start=time.time()\n",
    "print(\"value computed by TD0 \",TD0(PiThreshold, T))\n",
    "print(\"time elapsed is\",time.time()-start,\"\\n\")\n",
    "print(\"value computed by matrix inversion\",Values)\n",
    "# requires a large number of iterations to converge... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning \n",
    "\n",
    "Implement the Q-Learning algorithm, which aim at producing and estimate of $Q^\\star$ and the associated optimal policy $\\pi^\\star$. Try to display intermediate estimates of the policy in order to monitor the convergence to the optimal policy (which is slow...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
